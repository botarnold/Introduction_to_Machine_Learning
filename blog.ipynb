{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of support vector machine is a **maximized margin classifier** to search for a *optimal separating hyperplace* with the new dimension. We can call it as *decision boundary*. This article will discuss about the linear classifier for binary classification as example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "The binary classification mainly categorized the input data as 2 type, let's say +1 and -1. The support vector is to find a hyperplane (line) with the largest margin to split these two types of data, such that\n",
    "\n",
    "Two planes with maximized margin:\n",
    "$\\\\ H_1: x_i \\cdot w + b = +1$ \n",
    "$\\\\ H_2: x_i \\cdot w + b = -1$\n",
    "\n",
    "<img src=\"0.jpg\">\n",
    "\n",
    "w = the weight vector\\\n",
    "x = the input vector\\\n",
    "b = the bias\\\n",
    "d+ = the shortest distance to the closest positive point\\\n",
    "d- = the shortest distance to the closest negative point\\\n",
    "Concluded that the Margin (total distance between two planes) is $\\frac{2}{\\parallel w\\parallel}$\n",
    "\n",
    "##### Goal: Maximize margin / Minimize $\\parallel w\\parallel^2$ \n",
    "- Also need to satify $y^{i}f(x^{i})\\geq 1$ for all datapoints $(x^{i}, y^{i})$.\\\n",
    "- The problem is that minimizing $\\parallel w\\parallel^2$ is a quadratic function, which causes two constraints:\n",
    "    1. gradient constraint, i.e. solution is a max or a min.\n",
    "    2. solution is on the constraint line.\n",
    "    It can be solved by **Lagragian multiplier method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Errors\n",
    "After we found the optimal hyperplanes, some data errors still exists.\\\n",
    "If we insist to perfectly classify two datapoints, the margin would be too small, we would call it as hard margin (refer to below diagram). Also, for the machine learning model, it is probably **overfitting**.\\\n",
    "<img src=\"hard.jpg\">\\\n",
    "For general use of the model, the margin should be maximized and tolerated some errors. We call it soft margin.\\\n",
    "<img src=\"soft.jpg\">\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Margin Violations\n",
    "As we know, some datapoints are wrongly classified. Thus the optimization has been formulated as:\n",
    "$$\n",
    "\\min_{w \\in \\mathbb{R^{d}},\\xi_i \\in \\mathbb{R+}}\\parallel w \\parallel^2 + C\\sum_{i}^{N}\\xi_i \\\\$$\n",
    "\\\n",
    "suject to $$y_{i}(w^{T}x_{i}+b)\\geq 1 - \\xi_i $$\n",
    "\n",
    "for i=1...N and $\\xi_i \\geq 0\\\\$\n",
    "\n",
    "$$\n",
    "where \\ C\\sum_{i}^{N}\\xi_i \\ is \\ the \\ loss \\ function.\n",
    "$$ \n",
    "\n",
    "For the error points between hyperplane $H_1$ and $H_2$, $0 < \\xi_i < 1$.\\\n",
    "For the points in incorrect hyperplane ($H_1$ or $H_2$), $\\xi_i > 1$.\\\n",
    "For the points in correct hyperplane ($H_1$ or $H_2$), $\\xi_i = 0$.\n",
    "\n",
    "In other words, there are three cases:\n",
    "1. Point is outside margin. No contribution to loss.\n",
    "2. Point is on margin. No contribution to loss but in a hard margin.\n",
    "3. Point violates margin constraint. Contribute to loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problem\n",
    "* Does the cost function C have a unique solution? (local/global minimum or maximum)\n",
    "+ Does the solution depend on the starting point of an iterative optimization algorithm (such as gradient descent)?\n",
    "\n",
    "#### Idea: \n",
    "If the function is convex, then local minimum = global minimum $\\rightarrow$ A non-negative sum of convex functions is convex.\\\n",
    "<img src=\"convex.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this article, we breifly discuss about the support vector machine in mathematical way, i.e. the approach (maximizing margin) and listed out the method to solve quadratic function by Lagragian multiplier method. We consider the hyperplane cannot perfectly classify all datapoints in correct class. Our approach is to adopt soft margin to tolerate the error points, such that add a loss function to make adjustment for the optimization. The object is to aviod this algorithm tending to be overfitting or underfitting. At last, we bring out the issue of cost function and it will be continued in next article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d5c1156327dacead463cc502c55ebae8ce9c8c01979cf154173ff808e75bf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
